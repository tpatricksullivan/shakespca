{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Principal Components in Textual Analysis\n",
    "\n",
    "## Problem\n",
    "\n",
    "We would like to understand mutliple bodies of texts numerically and find a way to represent them that emphasizes the relevant differences. The hope is that this reveals previously unrecognized connections between diverse texts. \n",
    "\n",
    "The first step is to represent each text as a vector of numbers. This vector will almost necessarily contain _many_ dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Numeric Text Representation\n",
    "\n",
    "Our goal is to produce a $m\\times n$ matrix with $n$ variables for each of the $m$ texts. This matrix will have the form below, as an example.\n",
    "\n",
    "|Text|$v_1$|$\\ldots$|$v_j$|$\\ldots$|$v_n$|\n",
    "|--|--|--|--|--|\n",
    "|Text 1|5|...|100|...|9|\n",
    "|...|...|...|...|...|...|\n",
    "|Text m|10|...|87|...|3|\n",
    "\n",
    "Let us call this matrix $A$. We will return to it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### High Dimensionality\n",
    "There are two ways of representing each text numerically that I investigated. In _either_ case, the numeric representation of a text can have many hundreds or thousands of variables.\n",
    "\n",
    "| Option | Description | Advantages | Disadvantages |\n",
    "|--------|------------|---------------|---------------|\n",
    "|Word frequency|$v_j$ is the frequency of occurrences of a particular word|Non-parametric|Loses context|\n",
    "|Dictionary tagging|$v_j$ is the frequency of occurrences of a particular _set_ of words, usually chosen to have business meaning|Provides more business meaning|Parametric, requires a \"good\" dictionary to exist|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dimensionality Reduction\n",
    "We seek a way to reduce the many variables associated to a text to 2 or 3 significant variables that explain the key dimensions along which the text varies. \n",
    "\n",
    "If $A$ is our original matrix:\n",
    "\n",
    "|Text|$v_1$|$\\ldots$|$v_j$|$\\ldots$|$v_n$|\n",
    "|--|--|--|--|--|\n",
    "|Text 1|5|...|100|...|9|\n",
    "|...|...|...|...|...|...|\n",
    "|Text m|10|...|87|...|3|\n",
    "\n",
    "We seek a \"narrower\" version, perhaps with just 3 variables: \n",
    "\n",
    "|Text|$v'_1$|$v'_2$|$v'_3$|\n",
    "|--|--|--|--|--|\n",
    "|Text 1|5|34|100|\n",
    "|...|...|...|...|\n",
    "|Text m|10|47|87|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Shakespeare\n",
    "\n",
    "Our body of texts for this analysis will be the plays and sonnets of William Shakespeare.  His work is an ideal subject for this analysis. His plays vary widely from each other but return to the same themes, sometimes even with the same characters. At the same times, his audiences and readers recognized him as working in a few distinct genres. The First Folio, which was the first major printing to combine most of his works, grouped the plays into three genres: Comedies, Histories and Tragedies.\n",
    "* Comedies: All's Well That Ends Well, As You Like It, The Comedy of Errors, Cymbeline, Love's Labour's Lost, Measure for Measure, The Merchant of Venice, The Merry Wives of Windsor, A Midsummer Night's Dream, Much Ado About Nothing, Pericles, The Taming of the Shrew, The Tempest, Twelfth Night, The Two Gentlemen of Verona, The Two Noble Kinsmen, The Winter's Tale\n",
    "* Histories: 1 Henry IV, 2 Henry IV, Henry V, 1 Henry VI, 2 Henry VI, 3 Henry VI, Henry VIII, King John, Richard II, Richard III\n",
    "* Tragedies: Antony and Cleopatra, Coriolanus, Cymbeline, Hamlet, Julius Caesar, King Lear, Macbeth, Othello, Pericles, Romeo and Juliet, Timon of Athens, Titus Andronicus, Troilus and Cressida\n",
    "Our question will be to analyze what features of these plays make them \"Histories\", \"Tragedies\" or \"Comedies\". Perhaps we will even find a different and more revealing categorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Math\n",
    "A very simple, intuitive, and popular method of dimensionality reduction is Principle Component Analysis. \n",
    "\n",
    "### Principal Component Analysis (PCA)\n",
    "PCA creates new combinations of the variables in a way that represent most of the variance in the data. In particular, it is helpful to reduce a set of many variables into the key variables or combinations of variables that is most responsible for variances among texts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PCA Motivation\n",
    "#### Subtract the mean of each variable ####\n",
    "Let $A$ be the complete matrix of the samples and their features where we have standardized each vector of observations by subtracting its sample mean. Therefore, we assume that each column of the matrix $A$ has mean 0.\n",
    "$$\n",
    "A = \\begin{bmatrix} \n",
    "\\vdots & \\vdots &  \\vdots & \\vdots  & \\vdots \\\\\n",
    "\\mathbf{v_1} & \\ldots & \\mathbf{v_j} & \\ldots &  \\mathbf{v_n}  \\\\\n",
    "\\vdots & \\vdots &  \\vdots & \\vdots &  \\vdots \n",
    "\\end{bmatrix}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Find the covariance matrix of A by calculating $A^TA$ ####\n",
    "Note that one way to view each vector $\\mathbf{v_j}$ is as samples of a random variable $V_j$. Then the sample covariance of two variables $V_j$ and $V_k$ is: \n",
    "$$\n",
    "cov(V_j,V_k) = \\frac{1}{m-1} \\sum_{i=1}^m (v_{j,i} - \\bar{v_j})(v_{k,i} - \\bar{v_k})\n",
    "$$\n",
    "\n",
    "Note that since we have subtracted by the means, then $\\bar{v_j} = 0$.  Therefore, this is equivalent to the vector formulation: \n",
    "$$\n",
    "cov(V_j,V_k) = \\frac{1}{m-1} \\mathbf{v_j}^T \\mathbf{v_k}\n",
    "$$\n",
    "\n",
    "Finally, note that $A^TA$ is the matrix of covariances (after dividing by $m-1$)! The diagonal of this matrix is made up of variances of each $v_j$ and the off-diagonal entries are the \"cross-covariances\" between each pair of variables.\n",
    "\n",
    "$$\n",
    "\\frac{1}{m-1}\\mathbf{A}^T \\mathbf{A} = \\frac{1}{m-1}\\begin{bmatrix}\n",
    "\\mathbf{v_1}^T \\mathbf{v_1} & & \\vdots &   & \\mathbf{v_n}^T \\mathbf{v_1} \\\\\n",
    "\\vdots & \\ddots &  \\vdots &  & \\vdots  \\\\\n",
    "\\mathbf{v_1}^T \\mathbf{v_j} &  & \\mathbf{v_j}^T \\mathbf{v_j} &   &  \\mathbf{v_n}^T \\mathbf{v_j}  \\\\\n",
    "\\vdots &  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "\\mathbf{v_1}^T \\mathbf{v_n} &  & \\vdots  &  & \\mathbf{v_n}^T \\mathbf{v_n} \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Can we eliminate correlations between variables? ####\n",
    "Now that we know how to represent the covariances of our variables, we wish to find a transformation of the matrix $A$ into new variables that have no correlation with each other. In linear algebra terms, this means that the off-diagonal entries of the covariance matrix are reduced to zero.  This allows us to represent our samples in terms of $n$ variables with _no_ _cross-correlations_ _among_ _the_ _variables_!  In some sense, each of these new variables will explain a distinct driver of the variance in the data set and they aren't \"polluted\" by interaction with other variables.\n",
    "\n",
    "Is such a transformation possible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Calculate eigenvectors of $A^TA$ ####\n",
    "The answer is yes! This is where eigenvalues and eigenvectors appear.\n",
    "\n",
    "**Theorem** For any $m\\times n$ matrix, $A$, there exists a matrix $P$ such that $(AP)^T(AP)$ is a diagonal matrix.\n",
    "\n",
    "**Proof** We prove it by construction. Let $P$ be the matrix $S$ of eigenvectors ${x_j}$ of $A^TA$. Then, \n",
    "$$\n",
    "(AS)^T(AS) = S^T (A^T A) S\n",
    "$$\n",
    "Recall that $A^TA$ is a symmetric matrix. Therefore, we know that if $S$ is the matrix of eigenvectors of $A^TA$ and $\\Lambda$ the matrix of eigenvalues, then \n",
    "$$\n",
    "\\Lambda = S^T (A^T A) S\n",
    "$$\n",
    "Therefore, $S$ is the matrix we seek.\n",
    "\n",
    "Shlens, A Tutorial on Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PCA Procedure\n",
    "1. Standardize each vector of observations to have mean 0.  Call the matrix of standardized observations, $A$.\n",
    "2. Diagonalize the covariance matrix $A^TA = S \\Lambda S^T$. $S$ is the matrix whose columns are the eigenvectors of $A^TA$ and $\\Lambda$ is a diagonal matrix with the eigenvalues of $A^TA$ on the diagonal. Organize the eigenvectors of $A^TA$ from largest to smallest and rank the corresponding eigenvectors. These are your principal components.\n",
    "3. Transform $A$ to the new basis provided by calculating $AS$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PCA Connection to SVD\n",
    "In practice, the diagonalization of $A^TA$ is usually done with Singular Value Decomposition. In the SVD form of $A$, \n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{U} \\mathbf{D} \\mathbf{V}^T\n",
    "$$\n",
    "where $\\mathbf{D}$ is the diagonal matrix of eigenvalues of $A^TA$, and $V$ is the matrix of corresponding eigenvectors of $A^TA$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Code\n",
    "### Download the data\n",
    "I am using two main sources for the raw text files. The Shakespeare works come from [Folger Digital Texts][folger], and I downloaded a few other texts for comparison purposes from [Project Gutenberg][gutenberg]\n",
    "* Plays: [Folger Digital Texts][folger]\n",
    " * The Folger Digital Text library provides an API that gives \"stripped\" versions of the play. All of the stage directions, character names, and other non-spoken material are removed. The only text left is that spoken by characters in the play. I only did some minor cleaning to remove html tags.\n",
    "* Sonnets: [Folger Digital Texts][folger]\n",
    " * I did some minor cleaning to remove headings and other introductory material.\n",
    "* Pride and Prejudice: [Project Gutenberg][gutenberg-PandP]\n",
    " * The Project Gutenberg text has detailed disclaimers and other legal statements at the start and end of the text. I removed this material.\n",
    "* Leviathan: [Project Gutenberg][gutenberg-Leviathan]\n",
    " * This required similar cleaning as the Pride and Prejudice text.\n",
    "\n",
    "[folger]: http://www.folgerdigitaltexts.org\n",
    "[gutenberg]: http://www.gutenberg.org\n",
    "[gutenberg-PandP]: http://www.gutenberg.org/files/1342/1342-0.txt\n",
    "[gutenberg-Leviathan]: http://www.gutenberg.org/cache/epub/3207/pg3207.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%capture\n",
    "import download\n",
    "\n",
    "\n",
    "download.shakespeare_plays(\"playMetadata.csv\"); \n",
    "download.shakespeare_sonnets(); \n",
    "download.gutenberg_texts([('Pride and Prejudice', 'http://www.gutenberg.org/files/1342/1342-0.txt'),\n",
    "                      ('Leviathan', 'http://www.gutenberg.org/cache/epub/3207/pg3207.txt')]\n",
    "                     ); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dictionary Tagging: Tag data with categories\n",
    "[Docuscope] is a dictionary of words grouped by \"Language Action Types\" (LATs). There are mutliple versions. I used the open source one available on\n",
    "[GitHub][gh Docuscope].\n",
    "\n",
    "[Ubiqu-Ity][Ubiqu] is an academic tool for parsing texts that is specially written to work with Docuscope. It creates a vector for each text with the frequency of words used in each category. The source code is available from [GitHub][gh Ubiqu] under the BSD License. \n",
    "\n",
    "I execute the Docuscope tagging library through a shell script.\n",
    "\n",
    "[Docuscope]:http://www.cmu.edu/dietrich/english/research/docuscope.html\n",
    "[gh Docuscope]:https://github.com/docuscope/DocuScope-Dictionary-June-26-2012\n",
    "[Ubiqu]:http://vep.cs.wisc.edu/ubiq/\n",
    "[gh Ubiqu]:https://github.com/uwgraphics/Ubiqu-Ity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging corpus data...\n",
      "Starting tag_corpus...\n",
      "tag_corpus finished. Total elapsed time: 429.46 seconds.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "( cd ../Ubiqu-Ity-master && python ~/msca/Ubiqu-Ity-master/Ubiqu/tagCorpus.py --ngram_per_doc --ngram_count 1 --docuscope_version default ./data ./data_tagged )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Frequency\n",
    "Word frequency is usually analyzed through \"n-grams\". An n-gram is a sequence of $n$ words. For example, for $n=2$, the line \"to be or not to be\" is made up of the n-grams: to be, be or, or not, not to, to be. The Ubiqu+Ity utility will also count n-grams. For my analysis, I just counted 1-grams, which means I just counted usages of individual words.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load tagged data\n",
    "I wrote a few Python functions to load the tagged data and clean it properly. First we use the pandas module and represent the data as a dataframe.  Each row represents a play and the columns are categories of words. The numbers represent the share of the words in that play that fall in a given category. Note that the same words can appear in multiple categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AbstractConcepts</th>\n",
       "      <th>Acknowledge</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Apology</th>\n",
       "      <th>Aside</th>\n",
       "      <th>Attack_Citation</th>\n",
       "      <th>Authoritative_Citation</th>\n",
       "      <th>Autobio</th>\n",
       "      <th>Biographical_Time</th>\n",
       "      <th>Cause</th>\n",
       "      <th>...</th>\n",
       "      <th>SubjectivePercept</th>\n",
       "      <th>SubjectiveTime</th>\n",
       "      <th>Substitution</th>\n",
       "      <th>Support</th>\n",
       "      <th>TimeDate</th>\n",
       "      <th>TimeDuration</th>\n",
       "      <th>TimeShift</th>\n",
       "      <th>Transformation</th>\n",
       "      <th>Uncertainty</th>\n",
       "      <th>Updates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.015839</td>\n",
       "      <td>0.068395</td>\n",
       "      <td>0.295176</td>\n",
       "      <td>0.014399</td>\n",
       "      <td>0.140389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.341973</td>\n",
       "      <td>0.287977</td>\n",
       "      <td>0.187185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.957523</td>\n",
       "      <td>0.284377</td>\n",
       "      <td>0.017999</td>\n",
       "      <td>0.021598</td>\n",
       "      <td>0.021598</td>\n",
       "      <td>0.867531</td>\n",
       "      <td>0.226782</td>\n",
       "      <td>0.280778</td>\n",
       "      <td>0.323974</td>\n",
       "      <td>0.561555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.470064</td>\n",
       "      <td>0.091896</td>\n",
       "      <td>0.172654</td>\n",
       "      <td>0.005569</td>\n",
       "      <td>0.245057</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.448343</td>\n",
       "      <td>0.208855</td>\n",
       "      <td>0.094681</td>\n",
       "      <td>...</td>\n",
       "      <td>1.478697</td>\n",
       "      <td>0.281259</td>\n",
       "      <td>0.005569</td>\n",
       "      <td>0.013924</td>\n",
       "      <td>0.008354</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.323030</td>\n",
       "      <td>0.325815</td>\n",
       "      <td>0.587580</td>\n",
       "      <td>0.498468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.716238</td>\n",
       "      <td>0.160772</td>\n",
       "      <td>0.144695</td>\n",
       "      <td>0.008039</td>\n",
       "      <td>0.200965</td>\n",
       "      <td>0.008039</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.546624</td>\n",
       "      <td>0.454180</td>\n",
       "      <td>0.104502</td>\n",
       "      <td>...</td>\n",
       "      <td>1.571543</td>\n",
       "      <td>0.209003</td>\n",
       "      <td>0.020096</td>\n",
       "      <td>0.008039</td>\n",
       "      <td>0.024116</td>\n",
       "      <td>0.635048</td>\n",
       "      <td>0.462219</td>\n",
       "      <td>0.377814</td>\n",
       "      <td>0.454180</td>\n",
       "      <td>0.731511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.777649</td>\n",
       "      <td>0.134702</td>\n",
       "      <td>0.269404</td>\n",
       "      <td>0.018580</td>\n",
       "      <td>0.190441</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.222955</td>\n",
       "      <td>0.246180</td>\n",
       "      <td>0.074318</td>\n",
       "      <td>...</td>\n",
       "      <td>1.272702</td>\n",
       "      <td>0.176506</td>\n",
       "      <td>0.023224</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.004645</td>\n",
       "      <td>0.566678</td>\n",
       "      <td>0.301918</td>\n",
       "      <td>0.334433</td>\n",
       "      <td>0.469135</td>\n",
       "      <td>0.529518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.682472</td>\n",
       "      <td>0.044976</td>\n",
       "      <td>0.240941</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131714</td>\n",
       "      <td>0.009638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.282704</td>\n",
       "      <td>0.141352</td>\n",
       "      <td>0.109226</td>\n",
       "      <td>...</td>\n",
       "      <td>1.310717</td>\n",
       "      <td>0.212028</td>\n",
       "      <td>0.016063</td>\n",
       "      <td>0.019275</td>\n",
       "      <td>0.006425</td>\n",
       "      <td>0.462606</td>\n",
       "      <td>0.298766</td>\n",
       "      <td>0.221665</td>\n",
       "      <td>0.401568</td>\n",
       "      <td>0.815986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   AbstractConcepts  Acknowledge     Anger   Apology     Aside  \\\n",
       "0          2.015839     0.068395  0.295176  0.014399  0.140389   \n",
       "1          2.470064     0.091896  0.172654  0.005569  0.245057   \n",
       "2          1.716238     0.160772  0.144695  0.008039  0.200965   \n",
       "3          2.777649     0.134702  0.269404  0.018580  0.190441   \n",
       "4          2.682472     0.044976  0.240941  0.000000  0.131714   \n",
       "\n",
       "   Attack_Citation  Authoritative_Citation   Autobio  Biographical_Time  \\\n",
       "0         0.000000                     0.0  0.341973           0.287977   \n",
       "1         0.000000                     0.0  0.448343           0.208855   \n",
       "2         0.008039                     0.0  0.546624           0.454180   \n",
       "3         0.009290                     0.0  0.222955           0.246180   \n",
       "4         0.009638                     0.0  0.282704           0.141352   \n",
       "\n",
       "      Cause    ...     SubjectivePercept  SubjectiveTime  Substitution  \\\n",
       "0  0.187185    ...              0.957523        0.284377      0.017999   \n",
       "1  0.094681    ...              1.478697        0.281259      0.005569   \n",
       "2  0.104502    ...              1.571543        0.209003      0.020096   \n",
       "3  0.074318    ...              1.272702        0.176506      0.023224   \n",
       "4  0.109226    ...              1.310717        0.212028      0.016063   \n",
       "\n",
       "    Support  TimeDate  TimeDuration  TimeShift  Transformation  Uncertainty  \\\n",
       "0  0.021598  0.021598      0.867531   0.226782        0.280778     0.323974   \n",
       "1  0.013924  0.008354      0.701754   0.323030        0.325815     0.587580   \n",
       "2  0.008039  0.024116      0.635048   0.462219        0.377814     0.454180   \n",
       "3  0.009290  0.004645      0.566678   0.301918        0.334433     0.469135   \n",
       "4  0.019275  0.006425      0.462606   0.298766        0.221665     0.401568   \n",
       "\n",
       "    Updates  \n",
       "0  0.561555  \n",
       "1  0.498468  \n",
       "2  0.731511  \n",
       "3  0.529518  \n",
       "4  0.815986  \n",
       "\n",
       "[5 rows x 115 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cleanTaggedData\n",
    "import pandas as pd\n",
    "\n",
    "A, A_md = cleanTaggedData.get_tagged_texts()\n",
    "\n",
    "A.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load ngram data\n",
    "A second function loads a dataframe where each row is a play and each column the number of a given ngram used in that play. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>limited</th>\n",
       "      <th>o’erleaps</th>\n",
       "      <th>glamis</th>\n",
       "      <th>pardon</th>\n",
       "      <th>child</th>\n",
       "      <th>needful</th>\n",
       "      <th>foul</th>\n",
       "      <th>sleek</th>\n",
       "      <th>maggot</th>\n",
       "      <th>hath</th>\n",
       "      <th>...</th>\n",
       "      <th>gipsies</th>\n",
       "      <th>theou</th>\n",
       "      <th>certaintie</th>\n",
       "      <th>ecclesia</th>\n",
       "      <th>kinde</th>\n",
       "      <th>mulct</th>\n",
       "      <th>ascarides</th>\n",
       "      <th>coupling</th>\n",
       "      <th>jewes</th>\n",
       "      <th>reneweth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1H4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.033453</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>0.020908</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.213264</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1H6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009767</td>\n",
       "      <td>0.029301</td>\n",
       "      <td>0.004884</td>\n",
       "      <td>0.014651</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.253943</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2H4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015535</td>\n",
       "      <td>0.011651</td>\n",
       "      <td>0.003884</td>\n",
       "      <td>0.015535</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.256321</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2H6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.040925</td>\n",
       "      <td>0.008185</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.040925</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.249642</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3H6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.051553</td>\n",
       "      <td>0.034369</td>\n",
       "      <td>0.012888</td>\n",
       "      <td>0.030073</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.257765</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     limited  o’erleaps  glamis    pardon     child   needful      foul  \\\n",
       "1H4      NaN        NaN     NaN  0.033453  0.004182  0.004182  0.020908   \n",
       "1H6      NaN        NaN     NaN  0.009767  0.029301  0.004884  0.014651   \n",
       "2H4      NaN        NaN     NaN  0.015535  0.011651  0.003884  0.015535   \n",
       "2H6      NaN        NaN     NaN  0.040925  0.008185       NaN  0.040925   \n",
       "3H6      NaN        NaN     NaN  0.051553  0.034369  0.012888  0.030073   \n",
       "\n",
       "     sleek  maggot      hath    ...     gipsies  theou  certaintie  ecclesia  \\\n",
       "1H4    NaN     NaN  0.213264    ...         NaN    NaN         NaN       NaN   \n",
       "1H6    NaN     NaN  0.253943    ...         NaN    NaN         NaN       NaN   \n",
       "2H4    NaN     NaN  0.256321    ...         NaN    NaN         NaN       NaN   \n",
       "2H6    NaN     NaN  0.249642    ...         NaN    NaN         NaN       NaN   \n",
       "3H6    NaN     NaN  0.257765    ...         NaN    NaN         NaN       NaN   \n",
       "\n",
       "     kinde  mulct  ascarides  coupling  jewes  reneweth  \n",
       "1H4    NaN    NaN        NaN       NaN    NaN       NaN  \n",
       "1H6    NaN    NaN        NaN       NaN    NaN       NaN  \n",
       "2H4    NaN    NaN        NaN       NaN    NaN       NaN  \n",
       "2H6    NaN    NaN        NaN       NaN    NaN       NaN  \n",
       "3H6    NaN    NaN        NaN       NaN    NaN       NaN  \n",
       "\n",
       "[5 rows x 34561 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cleanTaggedData\n",
    "A_ngrams = cleanTaggedData.get_ngrams()\n",
    "A_ngrams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
